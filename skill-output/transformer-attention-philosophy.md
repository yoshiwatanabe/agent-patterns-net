# Transformer Attention: Emergent Relationships

## Algorithmic Philosophy

**Selective Resonance** is a visualization of how meaning flows through context via weighted relationships between sequential positions. In the transformer's attention mechanism, each token learns to distribute its focus across the entire sequence through learned query and key vectors—a process that creates dynamic relationship matrices. Rather than fixed pathways, attention establishes probabilistic connections: each position asks "what should I attend to?" and receives a distribution of answers, with some positions resonating strongly while others fade into background.

The visualization embodies this through an animated network where nodes represent sequential positions (tokens) and edges represent attention flows. The intensity and color of each edge encodes the attention weight—how strongly position i attends to position j. Multiple attention heads operate in parallel, each discovering different patterns of relationships: some heads focus locally (attending nearby positions), others learn to bridge distant dependencies, still others create specialized extraction patterns. The composition of all heads into a unified weighted sum reveals emergent semantic structure invisible in any single head.

The beauty emerges from controlled chaos: softmax normalization constrains each attention distribution to sum to 1, creating elegant probability landscapes. Attention weights flow as smooth probability gradients, not binary selections. The animation shows attention patterns crystallizing in real-time, with edge weights strengthening and weakening as if revealing underlying semantic magnetism. Brighter hues (oranges, whites) indicate strong focus; cooler tones (blues, grays) show distributed or negligible attention. The spatial layout maintains sequence position as a primary dimension, allowing viewers to read off which future positions current tokens attend to, what long-range dependencies exist, and how attention creates implicit hierarchies.

Parameters controlling sequence length, number of heads, and attention temperature allow exploration of how network scale and concentration affect pattern formation. Seeded randomness produces reproducible attention matrices—the same seed always reveals identical relationship structures, enabling deep study of how transformers internally organize and weight semantic significance. This is the computational substrate of language understanding: not memorized rules but learned probability distributions over relational patterns, elegantly visualized as flowing networks of selective focus and emergent connection.
